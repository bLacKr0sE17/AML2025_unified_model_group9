{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b16dbf63",
   "metadata": {},
   "source": [
    "### Unified Tabular Learning with LightGBM\n",
    "\n",
    "This notebook implements a single unified machine learning model trained jointly on three tabular datasets: HELOC, HIGGS, and CoverType.  \n",
    "The goal is to study whether a single model can generalize across heterogeneous tabular domains while remaining interpretable and robust.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "044592bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1f035e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Training Data\n",
    "heloc = pd.read_csv(\"heloc_train.csv\")\n",
    "covtype = pd.read_csv(\"covtype_train.csv\")\n",
    "higgs = pd.read_csv(\"higgs_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ab7a111e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Identifiers\n",
    "heloc[\"dataset_id\"] = 0\n",
    "covtype[\"dataset_id\"] = 1\n",
    "higgs[\"dataset_id\"] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "aea3daae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Target Encoding\n",
    "# HELOC\n",
    "heloc[\"target\"] = heloc[\"RiskPerformance\"].map({\n",
    "    \"Bad\": 0,\n",
    "    \"Good\": 1\n",
    "})\n",
    "\n",
    "# HIGGS\n",
    "higgs[\"target\"] = higgs[\"Label\"].map({\n",
    "    \"b\": 0,\n",
    "    \"s\": 1\n",
    "}) + 2\n",
    "\n",
    "# COVTYPE\n",
    "covtype[\"target\"] = covtype[\"Cover_Type\"] + 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2e48d104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Original Label Columns\n",
    "heloc = heloc.drop(columns=[\"RiskPerformance\"])\n",
    "higgs = higgs.drop(columns=[\"Label\"])\n",
    "covtype = covtype.drop(columns=[\"Cover_Type\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d19533d",
   "metadata": {},
   "source": [
    "### Dataset Integration and Unified Label Space\n",
    "\n",
    "Each dataset has different features, label definitions, and class distributions. To enable a single model, we align all feature spaces and construct a global label encoding so that all tasks can be learned jointly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f6017ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align Feature Spaces\n",
    "heloc_features = [c for c in heloc.columns if c not in [\"target\", \"dataset_id\"]]\n",
    "covtype_features = [c for c in covtype.columns if c not in [\"target\", \"dataset_id\"]]\n",
    "higgs_features = [c for c in higgs.columns if c not in [\"target\", \"dataset_id\"]]\n",
    "\n",
    "ALL_FEATURES = sorted(\n",
    "    set(heloc_features) |\n",
    "    set(covtype_features) |\n",
    "    set(higgs_features)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "62a3ce2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align feature spaces for training data\n",
    "def align_features(df):\n",
    "    df = df.copy()\n",
    "    for col in ALL_FEATURES:\n",
    "        if col not in df.columns:\n",
    "            df[col] = 0\n",
    "    return df[ALL_FEATURES + [\"dataset_id\", \"target\"]]\n",
    "\n",
    "\n",
    "# Apply feature alignment to training datasets\n",
    "heloc = align_features(heloc)\n",
    "covtype = align_features(covtype)\n",
    "higgs = align_features(higgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ed54cdae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine All Training Data\n",
    "full_data = pd.concat([heloc, covtype, higgs], ignore_index=True)\n",
    "\n",
    "X = full_data.drop(columns=[\"target\"])\n",
    "y = full_data[\"target\"]\n",
    "dataset_id = full_data[\"dataset_id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cdd2c4",
   "metadata": {},
   "source": [
    "### Handling Dataset and Class Imbalance\n",
    "\n",
    "The three datasets differ substantially in size and class balance. We experiment with multiple weighting strategies to prevent the largest dataset from dominating training and to improve performance on minority classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f769912c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Dataset + Class-Balanced Weights\n",
    "dataset_counts = full_data[\"dataset_id\"].value_counts().to_dict()\n",
    "class_counts = full_data[\"target\"].value_counts().to_dict()\n",
    "\n",
    "weights = []\n",
    "\n",
    "for _, row in full_data.iterrows():\n",
    "    d = int(row[\"dataset_id\"])\n",
    "    y_i = int(row[\"target\"])\n",
    "\n",
    "    w = 1.0 / dataset_counts[d]\n",
    "    w = w * (1.0 / class_counts[y_i])\n",
    "\n",
    "    weights.append(w)\n",
    "\n",
    "weights = np.array(weights, dtype=float)\n",
    "weights = weights / weights.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0f9a858c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine Best Number of Boosting Rounds\n",
    "X_train, X_val, y_train, y_val, d_train, d_val = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    dataset_id,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=dataset_id\n",
    ")\n",
    "\n",
    "w_train = weights[X_train.index]\n",
    "w_val = weights[X_val.index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "bb33c53d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[171]\tvalid_0's multi_error: 0.2162\n",
      "Best iteration: 171\n"
     ]
    }
   ],
   "source": [
    "train_data = lgb.Dataset(X_train, label=y_train, weight=w_train)\n",
    "val_data = lgb.Dataset(X_val, label=y_val, weight=w_val)\n",
    "\n",
    "params = {\n",
    "    \"objective\": \"multiclass\",\n",
    "    \"num_class\": 11,\n",
    "    \"metric\": \"multi_error\", # # multi_error = 1 - accuracy\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"num_leaves\": 63,\n",
    "    \"min_data_in_leaf\": 50,\n",
    "    \"feature_fraction\": 0.8,\n",
    "    \"bagging_fraction\": 0.8,\n",
    "    \"bagging_freq\": 1,\n",
    "    \"lambda_l2\": 1.0,\n",
    "    \"verbosity\": -1,\n",
    "    \"seed\": 42\n",
    "}\n",
    "\n",
    "model = lgb.train(\n",
    "    params,\n",
    "    train_data,\n",
    "    valid_sets=[val_data],\n",
    "    num_boost_round=2000,\n",
    "    callbacks=[lgb.early_stopping(50)]\n",
    ")\n",
    "\n",
    "best_iteration = model.best_iteration\n",
    "print(\"Best iteration:\", best_iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eec5788",
   "metadata": {},
   "source": [
    "### Final Model Selection\n",
    "\n",
    "After evaluating multiple weighting strategies and random seeds, we retrain the best-performing configuration on the full dataset to produce the final submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5c8631fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain Final Model on all data\n",
    "final_train_data = lgb.Dataset(X, label=y, weight=weights)\n",
    "\n",
    "final_model = lgb.train(\n",
    "    params,\n",
    "    final_train_data,\n",
    "    num_boost_round=best_iteration\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f550f369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Test Data\n",
    "heloc_test = pd.read_csv(\"heloc_test.csv\")\n",
    "covtype_test = pd.read_csv(\"covtype_test.csv\")\n",
    "higgs_test = pd.read_csv(\"higgs_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d4c61fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Test Sets\n",
    "def prepare_test(df, dataset_id):\n",
    "    df = df.copy()\n",
    "    df[\"dataset_id\"] = dataset_id\n",
    "    for col in ALL_FEATURES:\n",
    "        if col not in df.columns:\n",
    "            df[col] = 0\n",
    "    return df[ALL_FEATURES + [\"dataset_id\"]]\n",
    "\n",
    "heloc_test = prepare_test(heloc_test, 0)\n",
    "covtype_test = prepare_test(covtype_test, 1)\n",
    "higgs_test = prepare_test(higgs_test, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "40f654bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict Test Sets\n",
    "heloc_preds = np.argmax(final_model.predict(heloc_test), axis=1)\n",
    "covtype_preds = np.argmax(final_model.predict(covtype_test), axis=1)\n",
    "higgs_preds = np.argmax(final_model.predict(higgs_test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "92c8b264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode Predictions\n",
    "heloc_final = heloc_preds\n",
    "higgs_final = higgs_preds - 2\n",
    "covtype_final = covtype_preds - 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "30d02ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaggle submission\n",
    "final_submission = pd.concat([\n",
    "    pd.DataFrame({\n",
    "        \"ID\": np.arange(1, 1 + len(covtype_final)),\n",
    "        \"Prediction\": covtype_final\n",
    "    }),\n",
    "    pd.DataFrame({\n",
    "        \"ID\": np.arange(3501, 3501 + len(heloc_final)),\n",
    "        \"Prediction\": heloc_final\n",
    "    }),\n",
    "    pd.DataFrame({\n",
    "        \"ID\": np.arange(4547, 4547 + len(higgs_final)),\n",
    "        \"Prediction\": higgs_final\n",
    "    })\n",
    "], ignore_index=True)\n",
    "\n",
    "final_submission.to_csv(\n",
    "    \"final_submission_group9_unified_class_balanced_full.csv\",\n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1645025",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "In this notebook, we developed a single unified LightGBM model trained jointly on three heterogeneous tabular datasets: HELOC, HIGGS, and CoverType. To enable this, we aligned all feature spaces, constructed a global label encoding, and included a dataset identifier as an additional feature.\n",
    "\n",
    "We systematically evaluated several design choices, including different weighting strategies to address dataset and class imbalance, as well as multiple random seeds to assess robustness. Dataset-only and smoothed weighting strategies resulted in lower performance, while full dataset and class inverse-frequency weighting consistently achieved the best results.\n",
    "\n",
    "Our final model was selected based on both performance and stability and was retrained on the full training data before generating the submission. Overall, the results show that a single unified model can effectively learn across heterogeneous tabular domains when imbalance is handled carefully and experimental choices are evaluated in a controlled manner.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.13 (Applied ML)",
   "language": "python",
   "name": "applied-ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
